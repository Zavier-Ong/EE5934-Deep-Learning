{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "import sklearn\n",
    "print(\"scikit-learn version: {}\".format(sklearn.__version__))\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier # neural network\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model\n",
    "\n",
    "For this project, you will use MLP and a pre-trained deep neural network, [SqueezeNet](https://arxiv.org/abs/1602.07360), which is lightweight and runs fast on CPUs. Run the code below to load a pre-trained SqueezeNet from the PyTorch official model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and set the device.\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print('Use', device)\n",
    "\n",
    "# Download and load the pretrained SqueezeNet model.\n",
    "model = torchvision.models.squeezenet1_1(pretrained=True).to(device)\n",
    "\n",
    "# Disable the gradient computation with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For Task#1, use the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris). \n",
    "\n",
    "For Task#2, use the images in folder `Project1\\images` where the filenames are the corresponding class labels. For example, `182.png` is an image of Border Terrier, which is class 182 in ImageNet dataset. Please refer to [this Gist snippet](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) for a complete list. The images are from ImageNet validation set, and so the pre-trained model has never \"seen\" them.\n",
    "\n",
    "For Task#3, you may use the images in folder `Project1\\style` or any other images you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "Most pre-trained models are trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation. Here are a few helper functions for performing and undoing this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, size=(224, 224)):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN.tolist(),\n",
    "                    std=IMAGENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / IMAGENET_MEAN).tolist()),\n",
    "        T.Normalize(mean=(-IMAGENET_STD).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "\n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task#1 MLP Structure Search\n",
    "\n",
    "Multilayer Perceptrons, or MLPs for short, are the classical type of neural network. They are comprised of one or more layers of neurons. Data is fed to the input layer, there may be one or more hidden layers providing levels of abstraction, and predictions are made on the output layer, also called the visible layer. MLPs are suitable for classification prediction problems where inputs are assigned a class or label. However, the number of nodes in each hidder layer will affect the final prediction performance. \n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models. The general procedure is as follows:\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "   1. Take the group as a hold out or test data set\n",
    "   2. Take the remaining groups as a training data set\n",
    "   3. Fit a model on the training set and evaluate it on the test set\n",
    "   4. Retain the evaluation score and discard the model\n",
    "\n",
    "\n",
    "In this project, perform a 5-fold Cross-validation test on the Iris dataset to determine the best 5-layer MLP structure (from among 1 to 10 nodes in the layer) for test prediction. Provide a plot of the average 5-fold\n",
    "training and test accuracies over the different network structures.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_network_size(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Find the hidden node size that gives the best validation accuracy by 5-fold Cross-validation test, plots the average 5-fold \n",
    "    training and test accuracies over the different network structures.\n",
    "    \n",
    "    Inputs:\n",
    "    - X_train: Input Features; Tensor of shape (120, 4)\n",
    "    - y_train: An integer in the range [0, 3)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    - Nhidden: the size that gives the best validation accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    #The lists to store the training and validation accuracies. \n",
    "    acc_train_array = []\n",
    "    acc_valid_array = []\n",
    "    \n",
    "    for Nhidd in range(1,11):\n",
    "        #iteration to find the best node size \n",
    "    ##############################################################################\n",
    "    # TODO: Find the hidden node size that gives the best validation accuracy by #\n",
    "    #5-fold Cross-validation test.                                               #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "        \n",
    "      \n",
    "    ##############################################################################\n",
    "    # TODO: Plots  the average 5-fold training and test accuracies over the      #\n",
    "    #different network structures.                                               #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return Best_Nhidden\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "iris_dataset = load_iris()\n",
    "## split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], \n",
    "                                                        iris_dataset['target'], \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=0)\n",
    "## find the best hidden node size using only the training set\n",
    "Nhidden = find_network_size(X_train, y_train)\n",
    "print('best hidden node size =', Nhidden, 'based on 5-fold cross-validation on training set')\n",
    "\n",
    "## perform evaluation\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(Nhidden,Nhidden,Nhidden,Nhidden,Nhidden), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "## trained output\n",
    "y_test_predict = clf.predict(X_test)\n",
    "test_accuracy = metrics.accuracy_score(y_test_predict,y_test)\n",
    "\n",
    "print('test accuracy =', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task#2 Class Visualization\n",
    "By starting with a random noise image and performing gradient ascent on a target class, we can generate an image that the network will recognize as the target class. This idea was first presented in [2]; [3] extended this idea by suggesting several regularization techniques that can improve the quality of the generated image.\n",
    "\n",
    "Concretely, let $I$ be an image and let $y$ be a target class. Let $s_y(I)$ be the score that a convolutional network assigns to the image $I$ for class $y$; note that these are raw unnormalized scores, not class probabilities. We wish to generate an image $I^*$ that achieves a high score for the class $y$ by solving the problem\n",
    "\n",
    "$$\n",
    "I^* = \\arg\\max_I (s_y(I) - R(I))\n",
    "$$\n",
    "\n",
    "where $R$ is a (possibly implicit) regularizer (note the sign of $R(I)$ in the argmax: we want to minimize this regularization term). We can solve this optimization problem using gradient ascent, computing gradients with respect to the generated image. We will use (explicit) L2 regularization of the form\n",
    "\n",
    "$$\n",
    "R(I) = \\lambda \\|I\\|_2^2\n",
    "$$\n",
    "\n",
    "**and** implicit regularization as suggested by [3] by periodically blurring the generated image. We can solve this problem using gradient ascent on the generated image.\n",
    "\n",
    "In the cell below, complete the implementation of the `create_class_visualization` function.\n",
    "\n",
    "[2] [Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, \"Deep Inside Convolutional Networks: Visualising\n",
    "Image Classification Models and Saliency Maps\", ICLR Workshop 2014](https://arxiv.org/abs/1312.6034)\n",
    "\n",
    "[3] [Yosinski et al, \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop](https://arxiv.org/abs/1506.06579)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(X, ox, oy):\n",
    "    \"\"\"\n",
    "    Helper function to randomly jitter an image.\n",
    "  \n",
    "    Inputs\n",
    "    - X: PyTorch Tensor of shape (N, C, H, W)\n",
    "    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n",
    "    \n",
    "    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    if ox != 0:\n",
    "        left = X[:, :, :, :-ox]\n",
    "        right = X[:, :, :, -ox:]\n",
    "        X = torch.cat([right, left], dim=3)\n",
    "    if oy != 0:\n",
    "        top = X[:, :, :-oy]\n",
    "        bottom = X[:, :, -oy:]\n",
    "        X = torch.cat([bottom, top], dim=2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_visualization(target_y, model, device, **kwargs):\n",
    "    '''\n",
    "    Generate an image to maximize the score of target_y under a pretrained model.\n",
    "    \n",
    "    Inputs:\n",
    "    - target_y: A list of two elements, where the first value is an integer in the range [0, 1000) giving the index of the\n",
    "                class, and the second value is the name of the class.\n",
    "    - model: A pretrained CNN that will be used to generate the image\n",
    "    - dtype: Torch datatype to use for computations\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - l2_reg: Strength of L2 regularization on the image\n",
    "    - learning_rate: How big of a step to take\n",
    "    - num_iterations: How many iterations to use\n",
    "    - blur_every: How often to blur the image as an implicit regularizer\n",
    "    - max_jitter: How much to gjitter the image as an implicit regularizer\n",
    "    - show_every: How often to show the intermediate result\n",
    "    '''\n",
    "    model.to(device)\n",
    "    l2_reg = kwargs.pop('l2_reg', 1e-3)\n",
    "    learning_rate = kwargs.pop('learning_rate', 25)\n",
    "    num_iterations = kwargs.pop('num_iterations', 100)\n",
    "    blur_every = kwargs.pop('blur_every', 10)\n",
    "    max_jitter = kwargs.pop('max_jitter', 16)\n",
    "    show_every = kwargs.pop('show_every', 25)\n",
    "    \n",
    "    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n",
    "    img = torch.randn(1, 3, 224, 224).mul_(1.0).to(device).requires_grad_()\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        # Randomly jitter the image a bit; this gives slightly nicer results\n",
    "        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n",
    "        img.data.copy_(jitter(img.data, ox, oy))\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Use the model to compute the gradient of the score for the     #\n",
    "        # class target_y with respect to the pixels of the image, and make a   #\n",
    "        # gradient step on the image using the learning rate. Don't forget the #\n",
    "        # L2 regularization term!                                              #\n",
    "        # Be very careful about the signs of elements in your code.            #\n",
    "        ########################################################################\n",
    "        # your code\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "\n",
    "        # Undo the random jitter\n",
    "        img.data.copy_(jitter(img.data, -ox, -oy))\n",
    "\n",
    "        # As regularizer, clamp and periodically blur the image\n",
    "        for c in range(3):\n",
    "            lo = float(-IMAGENET_MEAN[c] / IMAGENET_STD[c])\n",
    "            hi = float((1.0 - IMAGENET_MEAN[c]) / IMAGENET_STD[c])\n",
    "            img.data[:, c].clamp_(min=lo, max=hi)\n",
    "        if t % blur_every == 0:\n",
    "            blur_image(img.data, sigma=0.5)\n",
    "\n",
    "        # Periodically show the image\n",
    "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
    "            plt.imshow(deprocess(img.data.clone().cpu()))\n",
    "            class_name = target_y[1]\n",
    "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
    "            plt.gcf().set_size_inches(4, 4)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return deprocess(img.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the implementation in the cell above, run the following cell to generate an image of a Tarantula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = [76, \"Tarantula\"]\n",
    "# target_y = 366 # Gorilla\n",
    "out = create_class_visualization(target_y, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task#3 Style Transfer\n",
    "\n",
    "Another task which is closely related to image gradients is style transfer which has become a \"cool\" application in deep learning for computer vision applications. You need to study and implement the style transfer technique presented in the following paper [4] where the general idea is to take two images (a content image and a style image), and produce a new image that reflects the content of one but the artistic \"style\" of the other.\n",
    "\n",
    "[4] [Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"Image style transfer using convolutional neural networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "\n",
    "Below is an example.\n",
    "\n",
    "![](style_transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the loss\n",
    "\n",
    "To perform style transfer, you will need to first formulate a special loss function that matches the content and style of each respective image in the feature space, and then perform gradient descent on the pixels of the image itself.\n",
    "\n",
    "The loss function contains two parts: **content loss** and **style loss**. Read the paper [4] for details about the losses and implement them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_weight, content_current, content_original):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - content_weight: Scalar giving the weighting for the content loss.\n",
    "    - content_current: features of the current image; this is a PyTorch Tensor of shape\n",
    "      (1, C_l, H_l, W_l).\n",
    "    - content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).\n",
    "    \n",
    "    Returns:\n",
    "    - scalar content loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ##############################################################################\n",
    "    # TODO: Implement content loss function                                      #\n",
    "    # Note: It should not be very much code (less than 10 lines)                 #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "\n",
    "def gram_matrix(features):\n",
    "    \"\"\"\n",
    "    Compute the normalized Gram matrix from features.\n",
    "    The Gram matrix will be used to compute style loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - features: PyTorch Tensor of shape (N, C, H, W) giving features for\n",
    "      a batch of N images.\n",
    "    \n",
    "    Returns:\n",
    "    - gram: PyTorch Tensor of shape (N, C, C) giving the\n",
    "      normalized Gram matrices for the N input images.\n",
    "    \"\"\"    \n",
    "    ##############################################################################\n",
    "    # TODO: Implement the normalized Gram matrix compuation function             #\n",
    "    # Note: It should not be very much code (less than 10 lines)                 #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "\n",
    "def style_loss(feats, style_layers, style_targets, style_weights):\n",
    "    \"\"\"\n",
    "    Computes the style loss at a set of layers.\n",
    "    \n",
    "    Inputs:\n",
    "    - feats: list of the features at every layer of the current image.\n",
    "    - style_layers: List of layer indices into feats giving the layers to include in the\n",
    "      style loss.\n",
    "    - style_targets: List of the same length as style_layers, where style_targets[i] is\n",
    "      a PyTorch Variable giving the Gram matrix of the source style image computed at\n",
    "      layer style_layers[i].\n",
    "    - style_weights: List of the same length as style_layers, where style_weights[i]\n",
    "      is a scalar giving the weight for the style loss at layer style_layers[i].\n",
    "      \n",
    "    Returns:\n",
    "    - style_loss: A PyTorch Tensor holding a scalar giving the style loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    ##############################################################################\n",
    "    # TODO: Implement style loss function                                        #\n",
    "    # Note: It should not be very much code (less than 10 lines)                 #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting them together\n",
    "\n",
    "With these loss functions, you can now build your style transfer model. Implement the function below to perform style transfer. To test the model, you can use the content and style images that we have provided in `Project1/style`, or improvise using any image you like. Please save your output images in the `Project1/style` folder.\n",
    "\n",
    "Design and carry out some experiments (on your own!) to analyse how **the choice of layers** and **the weights** will influence the output image. Write down your observations and analysis in the Markdown cell provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, content_layer, content_weight,\n",
    "                   style_layers, style_weights, max_iter):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    You may first resize the image to a small size for fast computation.\n",
    "    \n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - content_layer: an index indicating which layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "    - style_layers: list of indices indicating which layers to use for style loss\n",
    "    - style_weights: list of weights to use for each layer in style_layers\n",
    "    - max_iter: max iterations of gradient updates\n",
    "    \n",
    "    Returns:\n",
    "    - output_image: an image with content from the content_image and \n",
    "    style from the style image\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the function for style transfer.                           #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ############################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: 1. Choose one pair of images under 'Project1/style', and finish the  # \n",
    "#          neural style transfer task by calling the style_transfer function.#\n",
    "#       2. Show the 3 related images: content image, style image and the     # \n",
    "#          generated style-transferred image.                                #\n",
    "##############################################################################\n",
    "# your code\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
